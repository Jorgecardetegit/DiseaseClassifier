{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dhJ4DJHF4D_KlLsiKZN26zIBj-o4Fe5d",
      "authorship_tag": "ABX9TyMYbiEYoYa5mi6UQELPLFqG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorgecardetegit/DiseaseClassifier/blob/main/Simple_CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZGCFe_6vNZR",
        "outputId": "5377da9c-5bb6-4d9e-ed57-c065dd3c2458"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "QyqzOVmYb7Kd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from glob import glob\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_dir = \"/content/drive/MyDrive/Apple disease proyect/dataset\"\n",
        "categories = os.listdir(gen_dir)\n",
        "categories = [category.split('___')[1].lower() for category in categories]\n",
        "\n",
        "apple_scab = glob('/content/drive/MyDrive/Apple disease proyect/dataset/Apple___Apple_scab/*.JPG')\n",
        "black_rot = glob(\"/content/drive/MyDrive/Apple disease proyect/dataset/Apple___Black_rot/*.JPG\")\n",
        "cedar_apple_rust = glob(\"/content/drive/MyDrive/Apple disease proyect/dataset/Apple___Cedar_apple_rust/*.JPG\")\n",
        "healthy = glob(\"/content/drive/MyDrive/Apple disease proyect/dataset/Apple___healthy/*.JPG\")"
      ],
      "metadata": {
        "id": "opsMMf-c2rdS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocessing\n",
        "\n",
        "### Normalizing or Standardizing and rescaling\n",
        "\n",
        "All the images in the dataset have the same size so it is not strictly necessary to Standardize or even Normalize the dataset, nevertheless I will normalize the images in order to converge faster during training and improve generalization. I won´t standardize it.\n",
        "\n",
        "In addition the images have a size of 255*255 which is quite standard and suitable for the models I am going to fit so I won´t rescale the images.\n",
        "\n",
        "### Data Augmentation\n",
        "As stated in the EDA anlaysis the main problem in the dataset is the imbalance of data amount presented in each category. In order to deal with this issue I will use the method of data augmentation to artificially incraese the number of images in the categories with less images.\n",
        "\n",
        "Is important to be carefull with this method, since it can provoke overfitting. To start with I will use the following technique with each variable:\n",
        "\n",
        "- apple_scab: Apply mild to moderate augmentation. An increase of 20-50% in the number of images will be the starting point.\n",
        "\n",
        "- black_rot: Will also apply mild to moderate augmentation. An increase of 20-50% in the number of images will be the starting point.\n",
        "\n",
        "- cedar_apple_rust: Apply moderate to aggressive data augmentation to this class to increase the number of samples. I aim to at least double the number of images in this class\n",
        "\n",
        "- healthy: No augmentation will be applied for now.\n",
        "\n",
        "### Splitting ratios\n",
        "- Train: 70%\n",
        "- Validation: 15%\n",
        "- Test: 15%\n",
        "\n",
        "### Parameters definition\n",
        "\n",
        "- IMAGE_WIDTH = 255\n",
        "- IMAGE_HEIGHT = 255\n",
        "- NUM_CHANNELS = 3\n",
        "- BATCH_SIZE = 32\n",
        "- EPOCHS = 50"
      ],
      "metadata": {
        "id": "G3pw1VH-3Onk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation"
      ],
      "metadata": {
        "id": "_Jv85MI5GO0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentation():\n",
        "\n",
        "  def shuffleBatchCategory(self,category):\n",
        "      shuffled_dataset = random.sample(category, len(category))\n",
        "\n",
        "      return shuffled_dataset\n",
        "\n",
        "  def transformations(self, image_path):\n",
        "      image = tf.io.read_file(image_path)\n",
        "      image = tf.image.decode_image(image, channels=3)\n",
        "\n",
        "      image = tf.image.random_flip_left_right(image)\n",
        "      image = tf.image.rot90(image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
        "      # Add the transoformations here\n",
        "\n",
        "      return image\n",
        "\n",
        "  def augmentation(self,image_list, num_image=None):\n",
        "      if num_image is None:\n",
        "          num_image = len(image_list)\n",
        "\n",
        "      augmented_images = [self.transformations(image) for image in image_list]\n",
        "      combined_images = image_list + augmented_images\n",
        "\n",
        "      return combined_images\n",
        "\n",
        "  def save_images(self, image_list, output_dir):\n",
        "      os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "      for i, image in enumerate(image_list):\n",
        "          transformed_filename = f\"transformed_image_{i}.jpg\"            # Genera un nombre de archivo único con sufijo de transformación y extensión .jpg\n",
        "          file_path = os.path.join(output_dir, transformed_filename)\n",
        "\n",
        "          image = tf.cast(image, tf.uint8)                               # Convierte la imagen a formato uint8 (0-255) y la guarda como JPEG\n",
        "          encoded_image = tf.image.encode_jpeg(image)\n",
        "\n",
        "          with open(file_path, \"wb\") as f:                               # Escribe el archivo JPEG en el directorio de salida\n",
        "              f.write(encoded_image.numpy())\n",
        "\n",
        "  def concatenate(category1, category2, category3):\n",
        "      general_dataset = category1 + category2 + category3\n",
        "\n",
        "      return general_dataset\n"
      ],
      "metadata": {
        "id": "8Kx3p1EYw-zY"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DataAugmentor = DataAugmentation()\n",
        "\n",
        "# Apple_scab_augmentation----------------------------------------------------------------------------------------------------------------------------------\n",
        "shuffled_dataset_apple_scab = DataAugmentor.shuffleBatchCategory(category = apple_scab)\n",
        "\n",
        "augmented_dataset_apple_scab = DataAugmentor.augmentation(image_list = shuffled_dataset_apple_scab,num_image = round(len(shuffled_dataset_apple_scab)/2))\n",
        "\n",
        "DataAugmentor.save_images(image_list = augmented_dataset_apple_scab,\n",
        "                          output_dir =\"/content/drive/MyDrive/Apple disease proyect/dataset/Apple___Apple_scab\" )\n",
        "\n",
        "# Black_rot augmentation----------------------------------------------------------------------------------------------------------------------------------\n",
        "shuffled_dataset_black_rot = DataAugmentor.shuffleBatchCategory(category = black_rot)\n",
        "\n",
        "augmented_dataset_black_rot = DataAugmentor.augmentation(image_list = shuffled_dataset_black_rot, num_image = round(len(shuffled_dataset_black_rot)/2))\n",
        "\n",
        "DataAugmentor.save_images(image_list = augmented_dataset_black_rot,\n",
        "                         output_dir =\"/content/drive/MyDrive/Apple disease proyect/dataset/Apple___Black_rot\" )\n",
        "\n",
        "# Cedar_apple_rust augmentation---------------------------------------------------------------------------------------------------------------------------\n",
        "shuffled_dataset_cedar_apple_rust = DataAugmentor.shuffleBatchCategory(category = cedar_apple_rust)\n",
        "\n",
        "augmented_dataset_cedar_apple_rust = DataAugmentor.augmentation(image_list = shuffled_dataset_cedar_apple_rust)\n",
        "\n",
        "DataAugmentor.save_images(image_list = augmented_dataset_cedar_apple_rust,\n",
        "                          output_dir =\"/content/drive/MyDrive/Apple disease proyect/dataset/Apple___Cedar_apple_rust\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "hRX8eHShAm4T",
        "outputId": "d0f535d8-2ce8-4635-c9ce-7098ecf1e56e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-2b1447898788>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maugmented_dataset_apple_scab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataAugmentor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffled_dataset_apple_scab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_dataset_apple_scab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m DataAugmentor.save_images(image_list = augmented_dataset_apple_scab,\n\u001b[0m\u001b[1;32m      9\u001b[0m                           output_dir =\"/content/drive/MyDrive/Apple disease proyect/dataset/Apple___Apple_scab\" )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-e28eeedd5bd4>\u001b[0m in \u001b[0;36msave_images\u001b[0;34m(self, image_list, output_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m                               \u001b[0;31m# Convierte la imagen a formato uint8 (0-255) y la guarda como JPEG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m           \u001b[0mencoded_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_jpeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6656\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: {{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cast string to uint8 is not supported [Op:Cast] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "kcWDhTO3cEv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_HEIGHT = 255\n",
        "IMAGE_WIDTH = 255\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "tKmVfQgFHhZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(augmented_images, labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "7OR4MWJyw-10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE)\n",
        "\n",
        "# Conjunto de validación\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# Conjunto de prueba\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "-uil4PWKw-3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar el modelo"
      ],
      "metadata": {
        "id": "TKmQaFGbfnPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')  # 4 clases de salida\n",
        "])\n"
      ],
      "metadata": {
        "id": "zJQ_Pp8aK3oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8hupc6XOK3qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7F7aatl_K3sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBR0j9CdK3ui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}